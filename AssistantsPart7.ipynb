{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7\n",
    "\n",
    "Univeral code that we will use for the entire notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Create an instance of the OpenAI class\n",
    "# This assumes you have the OPENAI_API_KEY environment variable set\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Assistants, Threads, and Messages Review\n",
    "Let's create a new assistant, thread, and some messages for us to use later on and to review the code for creating them.\n",
    "\n",
    "### Creating an Assistant\n",
    "First, let's make an Assistant we can use to communicate with our run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant(id='asst_kTzVAfsliuawpoR9jpSCXtTd', created_at=1715546726, description=None, instructions='You are a helpful assistant.', metadata={'holds_threads': 'True', 'likes_threads': 'True', 'holds_messages': 'True', 'likes_messages': 'True'}, model='gpt-4-turbo', name='Run Tester Assistant', object='assistant', tools=[], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=None, file_search=None), top_p=1.0)\n",
      "\n",
      "\n",
      "\n",
      "Run Tester Assistant\n",
      "{'holds_threads': 'True', 'likes_threads': 'True', 'holds_messages': 'True', 'likes_messages': 'True'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create an assistant.\n",
    "assistant = client.beta.assistants.create(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    instructions=\"You are a helpful assistant.\",\n",
    "    name=\"Run Tester Assistant\",\n",
    "    metadata={\n",
    "        \"holds_threads\": \"True\",\n",
    "        \"likes_threads\": \"True\",\n",
    "        \"holds_messages\": \"True\",\n",
    "        \"likes_messages\": \"True\",\n",
    "    },\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    ")\n",
    "\n",
    "# Print the details of the created assistant to check the properties.\n",
    "print(assistant)\n",
    "print(\"\\n\\n\")\n",
    "print(assistant.name)\n",
    "print(assistant.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Thread\n",
    "Now, let's create a Thread that can be used to hold our messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread(id='thread_5F1SXQGcLp8JMfsSc2O3nfFX', created_at=1715546727, metadata={'user': 'abc123'}, object='thread', tool_resources=ToolResources(code_interpreter=None, file_search=None))\n"
     ]
    }
   ],
   "source": [
    "# Create a thread using the OpenAI API and store it in a variable\n",
    "# The metadata specifies a user identifier\n",
    "thread = client.beta.threads.create(\n",
    "    metadata={\n",
    "        \"user\": \"abc123\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Output the result of the thread creation to the console\n",
    "print(thread)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Message\n",
    "Finally, let's create a Message that we can go into the Thread for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message(id='msg_d44yF4oH11Fx713GMlpRmRel', assistant_id=None, attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='What is a penguin? Avoid repeating information.'), type='text')], created_at=1715546727, incomplete_at=None, incomplete_details=None, metadata={'key': 'value'}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_5F1SXQGcLp8JMfsSc2O3nfFX')\n",
      "\n",
      "\n",
      "msg_d44yF4oH11Fx713GMlpRmRel\n",
      "[TextContentBlock(text=Text(annotations=[], value='What is a penguin? Avoid repeating information.'), type='text')]\n",
      "What is a penguin? Avoid repeating information.\n",
      "user\n"
     ]
    }
   ],
   "source": [
    "# Create a message in a specific thread using the client's message creation method.\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,  # ID of the thread where the message will be posted\n",
    "    role=\"user\",  # Role of the entity posting the message\n",
    "    content=\"What is a penguin?\",  # The textual content of the message\n",
    "    metadata={\"key\": \"value\"}  # Additional data associated with the message in key-value pairs\n",
    ")\n",
    "\n",
    "# Print the entire message object to view its details.\n",
    "print(message)\n",
    "\n",
    "# Print a blank line for better readability of the output.\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print specific attributes of the message.\n",
    "print(message.id)  # The unique identifier of the message\n",
    "print(message.content)  # The content of the message\n",
    "print(message.content[0].text.value)  # Assuming 'content' is a list of text objects, print the value of the first one\n",
    "print(message.role)  # The role associated with the message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Runs\n",
    "Runs are the engine that makes things happen with the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(id='run_QKbVfc577O4ea1eBP36y1rNi', assistant_id='asst_kTzVAfsliuawpoR9jpSCXtTd', cancelled_at=None, completed_at=None, created_at=1715546727, expires_at=1715547327, failed_at=None, incomplete_details=None, instructions='You are a helpful assistant.', last_error=None, max_completion_tokens=None, max_prompt_tokens=None, metadata={}, model='gpt-4-turbo', object='thread.run', required_action=None, response_format='auto', started_at=None, status='queued', thread_id='thread_5F1SXQGcLp8JMfsSc2O3nfFX', tool_choice='auto', tools=[], truncation_strategy=TruncationStrategy(type='auto', last_messages=None), usage=None, temperature=1.0, top_p=1.0, tool_resources={})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run = client.beta.threads.runs.create(\n",
    "  assistant_id=assistant.id,\n",
    "  thread_id=thread.id,\n",
    ")\n",
    "\n",
    "print(run)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Run Status\n",
    "You can't just keep looking at the run result to get the current status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(id='run_QKbVfc577O4ea1eBP36y1rNi', assistant_id='asst_kTzVAfsliuawpoR9jpSCXtTd', cancelled_at=None, completed_at=None, created_at=1715546727, expires_at=1715547327, failed_at=None, incomplete_details=None, instructions='You are a helpful assistant.', last_error=None, max_completion_tokens=None, max_prompt_tokens=None, metadata={}, model='gpt-4-turbo', object='thread.run', required_action=None, response_format='auto', started_at=None, status='queued', thread_id='thread_5F1SXQGcLp8JMfsSc2O3nfFX', tool_choice='auto', tools=[], truncation_strategy=TruncationStrategy(type='auto', last_messages=None), usage=None, temperature=1.0, top_p=1.0, tool_resources={})\n",
      "\n",
      "\n",
      "queued\n"
     ]
    }
   ],
   "source": [
    "print(run)\n",
    "print(\"\\n\")\n",
    "print(run.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to get the run status by retrieving the run manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(id='run_QKbVfc577O4ea1eBP36y1rNi', assistant_id='asst_kTzVAfsliuawpoR9jpSCXtTd', cancelled_at=None, completed_at=None, created_at=1715546727, expires_at=1715547327, failed_at=None, incomplete_details=None, instructions='You are a helpful assistant.', last_error=None, max_completion_tokens=None, max_prompt_tokens=None, metadata={}, model='gpt-4-turbo', object='thread.run', required_action=None, response_format='auto', started_at=None, status='queued', thread_id='thread_5F1SXQGcLp8JMfsSc2O3nfFX', tool_choice='auto', tools=[], truncation_strategy=TruncationStrategy(type='auto', last_messages=None), usage=None, temperature=1.0, top_p=1.0, tool_resources={})\n",
      "\n",
      "\n",
      "run_QKbVfc577O4ea1eBP36y1rNi\n",
      "None\n",
      "queued\n"
     ]
    }
   ],
   "source": [
    "runstatus = client.beta.threads.runs.retrieve(\n",
    "    thread_id=thread.id,\n",
    "    run_id=run.id,\n",
    ")\n",
    "\n",
    "print(runstatus)\n",
    "print(\"\\n\")\n",
    "print(runstatus.id)\n",
    "print(runstatus.last_error)\n",
    "print(runstatus.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can poll the status manually with a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run status: queued\n",
      "Run status: in_progress\n",
      "Run status: in_progress\n",
      "Run status: in_progress\n",
      "Run status: in_progress\n",
      "Final status: completed\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "\n",
    "another_run = client.beta.threads.runs.create(\n",
    "  assistant_id=assistant.id,\n",
    "  thread_id=thread.id,\n",
    ")\n",
    "\n",
    "# Retrieve initial run status\n",
    "run_status = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=another_run.id)\n",
    "\n",
    "# Poll the status while it's still queued or in progress\n",
    "while run_status.status in [\"queued\", \"in_progress\"]:\n",
    "    # Log current status\n",
    "    print(f\"Run status: {run_status.status}\")\n",
    "    \n",
    "    # Wait for 1 second\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Retrieve the status again\n",
    "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=another_run.id)\n",
    "\n",
    "# Optionally, print the final status or any other detail\n",
    "print(\"Final status:\", run_status.status)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best option is to save yourself some code and just use the \"create_and_poll\" method to do the same thing. \n",
    "\n",
    "When interacting with the API some actions such as starting a Run and adding files to vector stores are asynchronous and take time to complete. The SDK includes helper functions which will poll the status until it reaches a terminal state and then return the resulting object. If an API method results in an action which could benefit from polling there will be a corresponding version of the method ending in '_and_poll'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(id='run_lXDSChTpnFObrPkoTf8M2EsC', assistant_id='asst_kTzVAfsliuawpoR9jpSCXtTd', cancelled_at=None, completed_at=1715546755, created_at=1715546751, expires_at=None, failed_at=None, incomplete_details=None, instructions='You are a helpful assistant.', last_error=None, max_completion_tokens=None, max_prompt_tokens=None, metadata={}, model='gpt-4-turbo', object='thread.run', required_action=None, response_format='auto', started_at=1715546751, status='completed', thread_id='thread_5F1SXQGcLp8JMfsSc2O3nfFX', tool_choice='auto', tools=[], truncation_strategy=TruncationStrategy(type='auto', last_messages=None), usage=Usage(completion_tokens=108, prompt_tokens=246, total_tokens=354), temperature=1.0, top_p=1.0, tool_resources={})\n",
      "\n",
      "\n",
      "run_lXDSChTpnFObrPkoTf8M2EsC\n",
      "completed\n"
     ]
    }
   ],
   "source": [
    "auto_run_and_poll = client.beta.threads.runs.create_and_poll(\n",
    "    assistant_id=assistant.id,\n",
    "    thread_id=thread.id,\n",
    ")\n",
    "\n",
    "print(auto_run_and_poll)\n",
    "print(\"\\n\")\n",
    "print(auto_run_and_poll.id)\n",
    "print(auto_run_and_poll.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course you will probably want the output from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "Penguins are flightless birds that have a highly adapted sense for aquatic life. Originating mostly in the Southern Hemisphere, especially in Antarctica, these birds are known for their distinctive black and white plumage which acts as camouflage while swimming. The white belly blends with the bright surface water, deceiving predators from below, and the black back masks them against the darker ocean depths when viewed from above.\n",
      "\n",
      "Penguins are excellent divers, adapting to capture their prey underwater, where they eat a variety of marine organisms including fish, squid, and krill. When on land, they have an upright stance and can often be seen waddling or sliding on their bellies across the ice. Penguins are also sociable animals, forming large colonies that help in mating, nesting, and protecting against predators.\n",
      "\n",
      "Their breeding behavior is notable: they lay eggs and raise their chicks on land in regions that can be exceedingly harsh. Parents often take turns warming the eggs and hunting for food. Penguins have a remarkable ability to return to their specific nesting site, even within large colonies.\n",
      "\n",
      "Despite being birds, penguins have dense bones that make floating difficult and swimming to depths easier, an advantage when hunting underwater. Their body temperature is regulated by a thick layer of fat beneath the skin, and their feathers are tightly packed to provide waterproofing and insulation.\n",
      "\n",
      "Conservation efforts are vital for penguins as their populations are affected by climate change, overfishing, and ocean pollution. Many species are considered vulnerable or endangered and require protections to ensure their survival.\n"
     ]
    }
   ],
   "source": [
    "# Create a single run \n",
    "fresh_run = client.beta.threads.runs.create_and_poll(\n",
    "    assistant_id=assistant.id,\n",
    "    thread_id=thread.id,\n",
    ")\n",
    "\n",
    "# Retrieve messages from the thread\n",
    "messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "\n",
    "# Get the latest assistant message \n",
    "latest_assistant_message = None\n",
    "for message in messages.data:\n",
    "    if message.role == 'assistant' and message.run_id == fresh_run.id:\n",
    "        latest_assistant_message = message\n",
    "        break\n",
    "\n",
    "# Print the latest response\n",
    "if latest_assistant_message:\n",
    "    print(\"Output:\\n\" + latest_assistant_message.content[0].text.value) \n",
    "else:\n",
    "    print(\"No assistant message found for the fresh run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Simple Streaming Run\n",
    "Instead of having to wait for the entire run to finish before getting a result, it is best to stream output to the user for interactive sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new run with the stream option set to True\n",
    "# Attempt to create a new run\n",
    "stream_run = client.beta.threads.runs.create(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# Continue processing the new run\n",
    "for event in stream_run:\n",
    "    if hasattr(event.data, 'status'):  # Check if 'status' is an attribute of the event data\n",
    "        print(event.data.id)\n",
    "        print(event.data.status)\n",
    "    else:\n",
    "        print(f\"Event ID: {event.data.id} does not have a status attribute.\")\n",
    "        print(event.data.delta)\n",
    "    print(\"---------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Thread Locks\n",
    "\n",
    "#### Notification of Failed Run\n",
    "You can't have multiple runs on a thread. To deal with situations where this might occur, you can check to see if a run is already taking place and get feedback on your failed run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a run to interfere with the other run\n",
    "interference_run = client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant.id,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "# Function to check if there is an active run\n",
    "def check_active_run(thread_id):\n",
    "    try:\n",
    "        # Fetch runs for the thread and check their status\n",
    "        active_runs = client.beta.threads.runs.list(thread_id=thread_id)\n",
    "        for run in active_runs.data:\n",
    "            if run.status in [\"in_progress\", \"queued\"]:\n",
    "                return run.id\n",
    "        return None\n",
    "    except OpenAIError as e:\n",
    "        print(f\"Failed to check runs: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Get the active run ID, if any\n",
    "active_run_id = check_active_run(thread.id)\n",
    "if active_run_id:\n",
    "    print(f\"Thread already has an active run: {active_run_id}. Please wait until it completes.\")\n",
    "else:\n",
    "    try:\n",
    "        # Attempt to create a new run if no active run exists\n",
    "        stream_run = client.beta.threads.runs.create(\n",
    "            thread_id=thread.id,\n",
    "            assistant_id=assistant.id,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        # Continue processing the new run\n",
    "        for event in stream_run:\n",
    "            if hasattr(event.data, 'status'):  # Check if 'status' is an attribute of the event data\n",
    "                print(event.data.id)\n",
    "                print(event.data.status)\n",
    "            else:\n",
    "                print(f\"Event ID: {event.data.id} does not have a status attribute.\")\n",
    "                print(event.data.delta)  # Ensure that event.data.delta exists or handle it similarly\n",
    "            print(\"---------------\\n\")\n",
    "\n",
    "    except OpenAIError as e:  # Handle generic OpenAIError if BadRequestError is not explicitly available\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        if \"already has an active run\" in str(e):\n",
    "            print(\"A run is already active on this thread. Please wait until it completes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Waiting Your Turn\n",
    "A better strategy is to check to see if there is already a run going on and just poll until the run is complete then do your run afterward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# create a run to interfere with the other run\n",
    "interference_run = client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant.id,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "# Function to check if there is an active run\n",
    "def check_active_run(thread_id):\n",
    "    try:\n",
    "        # Fetch runs for the thread and check their status\n",
    "        active_runs = client.beta.threads.runs.list(thread_id=thread_id)\n",
    "        for run in active_runs.data:\n",
    "            if run.status in [\"in_progress\", \"queued\"]:\n",
    "                return True\n",
    "        return False\n",
    "    except OpenAIError as e:\n",
    "        print(f\"Failed to check runs: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Wait until there is no active run\n",
    "while check_active_run(thread.id):\n",
    "    print(\"Waiting for the existing run to complete...\")\n",
    "    time.sleep(10)  # Wait for 10 seconds before checking again\n",
    "\n",
    "# Once no active runs are detected, proceed to create a new run\n",
    "try:\n",
    "    print(\"Creating a new run...\")\n",
    "    stream_run = client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant.id,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    # Continue processing the new run\n",
    "    for event in stream_run:\n",
    "        if hasattr(event.data, 'status'):  # Check if 'status' is an attribute of the event data\n",
    "            print(event.data.id)\n",
    "            print(event.data.status)\n",
    "        else:\n",
    "            print(f\"Event ID: {event.data.id} does not have a status attribute.\")\n",
    "            print(event.data.delta)  # Ensure that event.data.delta exists or handle it similarly\n",
    "        print(\"---------------\\n\")\n",
    "\n",
    "except OpenAIError as e:\n",
    "    print(f\"Error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying Assistants and Threads with Runs\n",
    "You can make changes in real-time to your Runs that override or modify existing setting on Assistants and Threads.\n",
    "\n",
    "### Runs and Assistants\n",
    "Let's see how we can make changes to the Assistant settings for our Runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant.\n",
      "\n",
      "\n",
      "You are a helpful assistant. That speaks like a pirate.\n",
      "\n",
      "\n",
      "Output:\n",
      "Ahoy there! Penguins be stout-bodied seabirds that make their homes in the Southern Hemisphere. They be splendiferous swimmers with wings turned flippers that propel 'em through the salty deep like no other. Decked out in their iconic black and white plumage, they cut a fine form, appearing in formal wear to any who glance their way.\n",
      "\n",
      "These buccaneers of the chilly waters dine heartily on seafood such as krill, fish, and the occasional squid, diving deep and bobbing back up like corks in the ocean vast. On land, they might waddle comedically, but in the brine, they be as graceful as any sea creature can claim to be.\n",
      "\n",
      "With colonies that rival the size of cities, sprawling on ice or rocky island coasts, penguins aren't just about the individual life; they band together for survival in the harsh climes of the Antarctic or any coastal area liberal with its chill. Be it the towering Emperor penguin or the smaller little blue, these birds be a marvel of the seas, ready to face life with a gusto that belies their unassuming size. So, let's give a hearty cheer for these sturdy sea-farers, who navigate the rough and tumble ocean waters with the ease of a seasoned mariner!\n",
      "\n",
      "\n",
      "You are a helpful assistant.\n"
     ]
    }
   ],
   "source": [
    "# Show the Assistant instructions\n",
    "print(assistant.instructions)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Create a run to add to Assistant\n",
    "additional_instruction_run = client.beta.threads.runs.create_and_poll(\n",
    "    assistant_id=assistant.id,\n",
    "    thread_id=thread.id,\n",
    "    additional_instructions=\"That speaks like a pirate.\",\n",
    ")\n",
    "\n",
    "print(additional_instruction_run.instructions)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Retrieve messages from the thread\n",
    "messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "\n",
    "# Get the latest assistant message \n",
    "latest_assistant_message = None\n",
    "for message in messages.data:\n",
    "    if message.role == 'assistant' and message.run_id == additional_instruction_run.id:\n",
    "        latest_assistant_message = message\n",
    "        break\n",
    "\n",
    "# Print the latest response\n",
    "if latest_assistant_message:\n",
    "    print(\"Output:\\n\" + latest_assistant_message.content[0].text.value.strip()) \n",
    "else:\n",
    "    print(\"No assistant message found for the run.\")\n",
    "\n",
    "# add some space\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "# print the original assistant instructions\n",
    "print(assistant.instructions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NormalProgramming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
