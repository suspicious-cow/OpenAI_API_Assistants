{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7\n",
    "\n",
    "Univeral code that we will use for the entire notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Create an instance of the OpenAI class\n",
    "# This assumes you have the OPENAI_API_KEY environment variable set\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Assistants, Threads, and Messages Review\n",
    "Let's create a new assistant, thread, and some messages for us to use later on and to review the code for creating them.\n",
    "\n",
    "### Creating an Assistant\n",
    "First, let's make an Assistant we can use to communicate with our run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant(id='asst_NrEdtjtl0i7boekETI16xpMQ', created_at=1715530621, description=None, instructions='You are a helpful assistant.', metadata={'holds_threads': 'True', 'likes_threads': 'True', 'holds_messages': 'True', 'likes_messages': 'True'}, model='gpt-4-turbo', name='Run Tester Assistant', object='assistant', tools=[], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=None, file_search=None), top_p=1.0)\n",
      "\n",
      "\n",
      "\n",
      "Run Tester Assistant\n",
      "{'holds_threads': 'True', 'likes_threads': 'True', 'holds_messages': 'True', 'likes_messages': 'True'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create an assistant.\n",
    "assistant = client.beta.assistants.create(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    instructions=\"You are a helpful assistant.\",\n",
    "    name=\"Run Tester Assistant\",\n",
    "    metadata={\n",
    "        \"holds_threads\": \"True\",\n",
    "        \"likes_threads\": \"True\",\n",
    "        \"holds_messages\": \"True\",\n",
    "        \"likes_messages\": \"True\",\n",
    "    },\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    ")\n",
    "\n",
    "# Print the details of the created assistant to check the properties.\n",
    "print(assistant)\n",
    "print(\"\\n\\n\")\n",
    "print(assistant.name)\n",
    "print(assistant.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Thread\n",
    "Now, let's create a Thread that can be used to hold our messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread(id='thread_Uvl90I7QlfwrV0IfVrbhQv91', created_at=1715533516, metadata={'user': 'abc123'}, object='thread', tool_resources=ToolResources(code_interpreter=None, file_search=None))\n"
     ]
    }
   ],
   "source": [
    "# Create a thread using the OpenAI API and store it in a variable\n",
    "# The metadata specifies a user identifier\n",
    "thread = client.beta.threads.create(\n",
    "    metadata={\n",
    "        \"user\": \"abc123\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Output the result of the thread creation to the console\n",
    "print(thread)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Message\n",
    "Finally, let's create a Message that we can go into the Thread for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message(id='msg_WOdQPdL0HjmDXgc6C7wwcMA9', assistant_id=None, attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='What is a penguin?'), type='text')], created_at=1715533526, incomplete_at=None, incomplete_details=None, metadata={'key': 'value'}, object='thread.message', role='user', run_id=None, status=None, thread_id='thread_Uvl90I7QlfwrV0IfVrbhQv91')\n",
      "\n",
      "\n",
      "msg_WOdQPdL0HjmDXgc6C7wwcMA9\n",
      "[TextContentBlock(text=Text(annotations=[], value='What is a penguin?'), type='text')]\n",
      "What is a penguin?\n",
      "user\n"
     ]
    }
   ],
   "source": [
    "# Create a message in a specific thread using the client's message creation method.\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,  # ID of the thread where the message will be posted\n",
    "    role=\"user\",  # Role of the entity posting the message\n",
    "    content=\"What is a penguin?\",  # The textual content of the message\n",
    "    metadata={\"key\": \"value\"}  # Additional data associated with the message in key-value pairs\n",
    ")\n",
    "\n",
    "# Print the entire message object to view its details.\n",
    "print(message)\n",
    "\n",
    "# Print a blank line for better readability of the output.\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print specific attributes of the message.\n",
    "print(message.id)  # The unique identifier of the message\n",
    "print(message.content)  # The content of the message\n",
    "print(message.content[0].text.value)  # Assuming 'content' is a list of text objects, print the value of the first one\n",
    "print(message.role)  # The role associated with the message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Runs\n",
    "Runs are the engine that makes things happen with the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(id='run_JDa8jztYhghywzCG4VLrloQQ', assistant_id='asst_NrEdtjtl0i7boekETI16xpMQ', cancelled_at=None, completed_at=None, created_at=1715533567, expires_at=1715534167, failed_at=None, incomplete_details=None, instructions='You are a helpful assistant.', last_error=None, max_completion_tokens=None, max_prompt_tokens=None, metadata={}, model='gpt-4-turbo', object='thread.run', required_action=None, response_format='auto', started_at=None, status='queued', thread_id='thread_Uvl90I7QlfwrV0IfVrbhQv91', tool_choice='auto', tools=[], truncation_strategy=TruncationStrategy(type='auto', last_messages=None), usage=None, temperature=1.0, top_p=1.0, tool_resources={})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run = client.beta.threads.runs.create(\n",
    "  assistant_id=assistant.id,\n",
    "  thread_id=thread.id,\n",
    ")\n",
    "\n",
    "print(run)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Run Status\n",
    "You can't just keep looking at the run result to get the current status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(id='run_JDa8jztYhghywzCG4VLrloQQ', assistant_id='asst_NrEdtjtl0i7boekETI16xpMQ', cancelled_at=None, completed_at=None, created_at=1715533567, expires_at=1715534167, failed_at=None, incomplete_details=None, instructions='You are a helpful assistant.', last_error=None, max_completion_tokens=None, max_prompt_tokens=None, metadata={}, model='gpt-4-turbo', object='thread.run', required_action=None, response_format='auto', started_at=None, status='queued', thread_id='thread_Uvl90I7QlfwrV0IfVrbhQv91', tool_choice='auto', tools=[], truncation_strategy=TruncationStrategy(type='auto', last_messages=None), usage=None, temperature=1.0, top_p=1.0, tool_resources={})\n",
      "\n",
      "\n",
      "queued\n"
     ]
    }
   ],
   "source": [
    "print(run)\n",
    "print(\"\\n\")\n",
    "print(run.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to get the run status by retrieving the run manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(id='run_JDa8jztYhghywzCG4VLrloQQ', assistant_id='asst_NrEdtjtl0i7boekETI16xpMQ', cancelled_at=None, completed_at=1715533588, created_at=1715533567, expires_at=None, failed_at=None, incomplete_details=None, instructions='You are a helpful assistant.', last_error=None, max_completion_tokens=None, max_prompt_tokens=None, metadata={}, model='gpt-4-turbo', object='thread.run', required_action=None, response_format='auto', started_at=1715533567, status='completed', thread_id='thread_Uvl90I7QlfwrV0IfVrbhQv91', tool_choice='auto', tools=[], truncation_strategy=TruncationStrategy(type='auto', last_messages=None), usage=Usage(completion_tokens=403, prompt_tokens=36, total_tokens=439), temperature=1.0, top_p=1.0, tool_resources={})\n",
      "\n",
      "\n",
      "run_JDa8jztYhghywzCG4VLrloQQ\n",
      "None\n",
      "completed\n"
     ]
    }
   ],
   "source": [
    "runstatus = client.beta.threads.runs.retrieve(\n",
    "    thread_id=thread.id,\n",
    "    run_id=run.id,\n",
    ")\n",
    "\n",
    "print(runstatus)\n",
    "print(\"\\n\")\n",
    "print(runstatus.id)\n",
    "print(runstatus.last_error)\n",
    "print(runstatus.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can poll the status manually with a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run status: queued\n",
      "Run status: in_progress\n",
      "Run status: in_progress\n",
      "Run status: in_progress\n",
      "Run status: in_progress\n",
      "Run status: in_progress\n",
      "Run status: in_progress\n",
      "Run status: in_progress\n",
      "Run status: in_progress\n",
      "Run status: in_progress\n",
      "Run status: in_progress\n",
      "Run status: in_progress\n",
      "Run status: in_progress\n",
      "Run status: in_progress\n",
      "Run status: in_progress\n",
      "Final status: completed\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "\n",
    "another_run = client.beta.threads.runs.create(\n",
    "  assistant_id=assistant.id,\n",
    "  thread_id=thread.id,\n",
    ")\n",
    "\n",
    "# Retrieve initial run status\n",
    "run_status = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=another_run.id)\n",
    "\n",
    "# Poll the status while it's still queued or in progress\n",
    "while run_status.status in [\"queued\", \"in_progress\"]:\n",
    "    # Log current status\n",
    "    print(f\"Run status: {run_status.status}\")\n",
    "    \n",
    "    # Wait for 1 second\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Retrieve the status again\n",
    "    run_status = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=another_run.id)\n",
    "\n",
    "# Optionally, print the final status or any other detail\n",
    "print(\"Final status:\", run_status.status)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best option is to save yourself some code and just use the \"create_and_poll\" method to do the same thing. \n",
    "\n",
    "When interacting with the API some actions such as starting a Run and adding files to vector stores are asynchronous and take time to complete. The SDK includes helper functions which will poll the status until it reaches a terminal state and then return the resulting object. If an API method results in an action which could benefit from polling there will be a corresponding version of the method ending in '_and_poll'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(id='run_oTUOQy94rGjal8hk2m91qOaK', assistant_id='asst_NrEdtjtl0i7boekETI16xpMQ', cancelled_at=None, completed_at=1715537937, created_at=1715537921, expires_at=None, failed_at=None, incomplete_details=None, instructions='You are a helpful assistant.', last_error=None, max_completion_tokens=None, max_prompt_tokens=None, metadata={}, model='gpt-4-turbo', object='thread.run', required_action=None, response_format='auto', started_at=1715537921, status='completed', thread_id='thread_Uvl90I7QlfwrV0IfVrbhQv91', tool_choice='auto', tools=[], truncation_strategy=TruncationStrategy(type='auto', last_messages=None), usage=Usage(completion_tokens=351, prompt_tokens=1382, total_tokens=1733), temperature=1.0, top_p=1.0, tool_resources={})\n",
      "\n",
      "\n",
      "run_oTUOQy94rGjal8hk2m91qOaK\n",
      "completed\n"
     ]
    }
   ],
   "source": [
    "auto_run_and_poll = client.beta.threads.runs.create_and_poll(\n",
    "    assistant_id=assistant.id,\n",
    "    thread_id=thread.id,\n",
    ")\n",
    "\n",
    "print(auto_run_and_poll)\n",
    "print(\"\\n\")\n",
    "print(auto_run_and_poll.id)\n",
    "print(auto_run_and_poll.status)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NormalProgramming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
