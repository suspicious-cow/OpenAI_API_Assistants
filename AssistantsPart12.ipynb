{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 12\n",
    "\n",
    "# Using File Search\n",
    "\n",
    "Universal code for the entire notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to make sure you have all the packages needed\n",
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from openai import OpenAI  # Used for interacting with OpenAI's API\n",
    "from typing_extensions import override  # Used for overriding methods in subclasses\n",
    "from openai import AssistantEventHandler  # Used for handling events related to OpenAI assistants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the OpenAI class to interact with the API.\n",
    "# This assumes you have set the OPENAI_API_KEY environment variable.\n",
    "client = OpenAI() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event handler class to handle events related to streaming output from the assistant\n",
    "class EventHandler(AssistantEventHandler):\n",
    "    @override\n",
    "    def on_text_created(self, text) -> None:\n",
    "        print(f\"\\nASSISTANT MESSAGE >\\n\", end=\"\", flush=True)\n",
    "\n",
    "    @override\n",
    "    def on_tool_call_created(self, tool_call):\n",
    "        print(f\"\\nASSISTANT MESSAGE >\\n{tool_call.type}\\n\", flush=True)\n",
    "\n",
    "    @override\n",
    "    def on_message_done(self, message) -> None:\n",
    "        # print a citation to the file searched\n",
    "        message_content = message.content[0].text\n",
    "        annotations = message_content.annotations\n",
    "        citations = []\n",
    "        for index, annotation in enumerate(annotations):\n",
    "            message_content.value = message_content.value.replace(\n",
    "                annotation.text, f\"[{index}]\"\n",
    "            )\n",
    "            if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "                cited_file = client.files.retrieve(file_citation.file_id)\n",
    "                citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "        print(message_content.value)\n",
    "        print(\"\\n\".join(citations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload files for later use\n",
    "alice_file = client.files.create(file=open(\"./artifacts/Alice_in_Wonderland.pdf\",\"rb\"), purpose=\"assistants\")\n",
    "oz_file = client.files.create(file=open(\"./artifacts/The_Wonderful_Wizard_of_Oz.txt\",\"rb\"), purpose=\"assistants\")\n",
    "dracula_file = client.files.create(file=open(\"./artifacts/Dracula.pdf\",\"rb\"), purpose=\"assistants\")\n",
    "frank_file = client.files.create(file=open(\"./artifacts/Frankenstein.pdf\",\"rb\"), purpose=\"assistants\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice_in_Wonderland.pdf\n",
      "file-vb4MXmRqfWAWPUAQkChTXrlH\n",
      "\n",
      "\n",
      "The_Wonderful_Wizard_of_Oz.txt\n",
      "file-YXsvVm7pk4B4mwRFE4aTLvuV\n",
      "\n",
      "\n",
      "Dracula.pdf\n",
      "file-N5D0VEVMGNJVLCJGBF90Foga\n",
      "\n",
      "\n",
      "Frankenstein.pdf\n",
      "file-fy6wM1BdyPy0iCoAGZE5nWiy\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(alice_file.filename)\n",
    "print(alice_file.id)\n",
    "print(\"\\n\")\n",
    "print(oz_file.filename)\n",
    "print(oz_file.id)\n",
    "print(\"\\n\")\n",
    "print(dracula_file.filename)\n",
    "print(dracula_file.id)\n",
    "print(\"\\n\")\n",
    "print(frank_file.filename)\n",
    "print(frank_file.id)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Assistant with File Search enabled\n",
    "\n",
    "Our first step is to create an Assistant that can do file searching regardless of where the vector store resides (Assistant or Thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant(id='asst_DKHpJRFNW5QJGq5QMOu9KXT0', created_at=1717547443, description=None, instructions=' \\n        You are a helpful assistant that answers questions about the stories in your files. The stories are from a variety of authors. \\n        You will answer questions from the user about the stories. All you will do is answer questions about the stories in the files and provide related information.\\n        If the user asks you a question that is not related to the stories in the files, you should let them know that you can only answer questions about the stories.\\n    ', metadata={'can_be_used_for_file_search': 'True', 'can_hold_vector_store': 'True'}, model='gpt-4o', name='File Search Demo Assistant - Stories', object='assistant', tools=[FileSearchTool(type='file_search')], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=None, file_search=ToolResourcesFileSearch(vector_store_ids=[])), top_p=1.0)\n",
      "\n",
      "\n",
      "\n",
      "File Search Demo Assistant - Stories\n",
      "{'can_be_used_for_file_search': 'True', 'can_hold_vector_store': 'True'}\n"
     ]
    }
   ],
   "source": [
    "# Create an assistant using the client library.\n",
    "assistant = client.beta.assistants.create(\n",
    "    model=\"gpt-4o\",  # Specify the model to be used.\n",
    "    \n",
    "    instructions=\"\"\" \n",
    "        You are a helpful assistant that answers questions about the stories in your files. The stories are from a variety of authors. \n",
    "        You will answer questions from the user about the stories. All you will do is answer questions about the stories in the files and provide related information.\n",
    "        If the user asks you a question that is not related to the stories in the files, you should let them know that you can only answer questions about the stories.\n",
    "    \"\"\",\n",
    "    \n",
    "    name=\"File Search Demo Assistant - Stories\",  # Give the assistant a name.\n",
    "    \n",
    "    tools=[{\"type\": \"file_search\"}], # Add the file search capability to the assistant.\n",
    "    \n",
    "    metadata={  # Add metadata about the assistant's capabilities.\n",
    "        \"can_be_used_for_file_search\": \"True\",\n",
    "        \"can_hold_vector_store\": \"True\",\n",
    "    },\n",
    "    temperature=1,  # Set the temperature for response variability.\n",
    "    top_p=1,  # Set the top_p for nucleus sampling.\n",
    ")\n",
    "\n",
    "# Print the details of the created assistant to check its properties.\n",
    "print(assistant)  # Print the full assistant object.\n",
    "print(\"\\n\\n\")\n",
    "print(assistant.name)  # Print the name of the assistant.\n",
    "print(assistant.metadata)  # Print the metadata of the assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Vector Store\n",
    "\n",
    "Now we will create our vector store to hold our files and add files at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great Fiction Stories\n",
      "vs_NdLJjQQkuOXwlyek9M6qLKcb\n",
      "completed\n",
      "FileCounts(cancelled=0, completed=2, failed=0, in_progress=0, total=2)\n"
     ]
    }
   ],
   "source": [
    "from contextlib import ExitStack\n",
    "\n",
    "# Create a vector store with a name for the store.\n",
    "vector_store = client.beta.vector_stores.create(name=\"Great Fiction Stories\")\n",
    "\n",
    "# Ready the files for upload to the vector store.\n",
    "file_paths = [\"./artifacts/I_Am_Legend.pdf\", \"./artifacts/The_Veldt.pdf\"]\n",
    "\n",
    "# Using ExitStack to manage multiple context managers and ensure they are properly closed.\n",
    "with ExitStack() as stack:\n",
    "    # Open each file in binary read mode and add the file stream to the list\n",
    "    file_streams = [stack.enter_context(open(path, \"rb\")) for path in file_paths]\n",
    "\n",
    "    # Use the upload and poll helper method to upload the files, add them to the vector store,\n",
    "    # and poll the status of the file batch for completion.\n",
    "    file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "        vector_store_id=vector_store.id, files=file_streams\n",
    "    )\n",
    "\n",
    "    # Print the vector store information\n",
    "    print(vector_store.name)\n",
    "    print(vector_store.id)\n",
    "    \n",
    "    # Print the status and the file counts of the batch to see the results\n",
    "    print(file_batch.status)\n",
    "    print(file_batch.file_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attaching the Vector Store to the Assistant\n",
    "\n",
    "We have an Assistant that has File Search enabled and we have a Vector Store with files in them. It's time to join the two up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant Tools:\n",
      " - FileSearchTool(type='file_search')\n",
      "\n",
      "Assistant Tool Resources:\n",
      " - code_interpreter: None\n",
      " - file_search: ToolResourcesFileSearch(vector_store_ids=['vs_NdLJjQQkuOXwlyek9M6qLKcb'])\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Attach the vector store to the assistant to enable file search capabilities.\n",
    "    assistant = client.beta.assistants.update(\n",
    "        assistant_id=assistant.id,\n",
    "        tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    "    )\n",
    "\n",
    "    # Print the assistant's tools and tool resources to verify the attachment of the vector store.\n",
    "    print(\"Assistant Tools:\")\n",
    "    for tool in assistant.tools:\n",
    "        print(f\" - {tool}\")\n",
    "\n",
    "    # Print the assistant's tool resources to verify the attachment of the vector store\n",
    "    print(\"\\nAssistant Tool Resources:\")\n",
    "    for resource, details in assistant.tool_resources:\n",
    "        print(f\" - {resource}: {details}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while updating the assistant: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Assistant and Vector Store at the Same Time\n",
    "\n",
    "If we have file id's we can just feed them in when creating an assistant to get the Assistant and the Vector Store at the same time using the vector_stores option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant(id='asst_C4gJGXXPg1LzP5MAPE0bqD4Q', created_at=1717547755, description=None, instructions='You are a helpful assistant that answers questions about the stories in your files. The stories are from a variety of authors. You will answer questions from the user about the stories. All you will do is answer questions about the stories in the files and provide related information. If the user asks you a question that is not related to the stories in the files, you should let them know that you can only answer questions about the stories.', metadata={'can_be_used_for_file_search': 'True', 'has_vector_store': 'True'}, model='gpt-4o', name='Quick Assistant and Vector Store at Once', object='assistant', tools=[FileSearchTool(type='file_search')], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=None, file_search=ToolResourcesFileSearch(vector_store_ids=['vs_c9HyAfF8ZEhbgj6UNprf72Qf'])), top_p=1.0)\n",
      "\n",
      "\n",
      "\n",
      "Assistant Name: Quick Assistant and Vector Store at Once\n",
      "\n",
      "\n",
      "Vector Store Name: None\n",
      "Vector Store Id: vs_c9HyAfF8ZEhbgj6UNprf72Qf\n",
      "Vector Store Metadata: {'Book1': 'Wizard of Oz', 'Book2': 'Alice in Wonderland'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create an assistant using the client library.\n",
    "try:\n",
    "    assistant_with_vector_store = client.beta.assistants.create(\n",
    "        model=\"gpt-4o\",  # Specify the model to be used.\n",
    "        instructions=(\n",
    "            \"You are a helpful assistant that answers questions about the stories in your files. \"\n",
    "            \"The stories are from a variety of authors. \"\n",
    "            \"You will answer questions from the user about the stories. All you will do is answer questions about the stories in the files and provide related information. \"\n",
    "            \"If the user asks you a question that is not related to the stories in the files, you should let them know that you can only answer questions about the stories.\"\n",
    "        ),\n",
    "        name=\"Quick Assistant and Vector Store at Once\",  # Give the assistant a name.\n",
    "        tools=[{\"type\": \"file_search\"}],  # Add the file search capability to the assistant.\n",
    "        # Create a vector store and attach it to the assistant in one step.\n",
    "        tool_resources={\n",
    "            \"file_search\": {\n",
    "                \"vector_stores\": [\n",
    "                    {\n",
    "                        \"file_ids\": [\n",
    "                            oz_file.id,\n",
    "                            alice_file.id\n",
    "                        ],\n",
    "                        \"metadata\": {\n",
    "                            \"Book1\": \"Wizard of Oz\", \n",
    "                            \"Book2\": \"Alice in Wonderland\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        metadata={  # Add metadata about the assistant's capabilities.\n",
    "            \"can_be_used_for_file_search\": \"True\",\n",
    "            \"has_vector_store\": \"True\",\n",
    "        },\n",
    "        temperature=1,  # Set the temperature for response variability.\n",
    "        top_p=1,  # Set the top_p for nucleus sampling.\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while creating the assistant: {e}\")\n",
    "else:\n",
    "    # Print the details of the created assistant to check its properties.\n",
    "    print(assistant_with_vector_store)  # Print the full assistant object.\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"Assistant Name: \" + assistant_with_vector_store.name)  # Print the name of the assistant.\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # get the vector store information\n",
    "    unnamed_assistant_vector_store = client.beta.vector_stores.retrieve(assistant_with_vector_store.tool_resources.file_search.vector_store_ids[0])\n",
    "    print(\"Vector Store Name: \" + str(unnamed_assistant_vector_store.name))\n",
    "    print(\"Vector Store Id: \" + unnamed_assistant_vector_store.id)\n",
    "    print(\"Vector Store Metadata: \" + str(unnamed_assistant_vector_store.metadata))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Threads with Vector Stores\n",
    "\n",
    "### Creating Vector Stores with Thread Messages\n",
    "\n",
    "You can create a vector store in threads with one of two ways: messages or during thread creation. If you create a thread with a vector store then it will also be searched when looking for information during the run. First, let's take a look at the most common scenario, vector stores created with messages in the thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vs_V3zna6JcBu8AEDxxk8vt0BoA\n"
     ]
    }
   ],
   "source": [
    "# We assume that the user has given us a file to upload\n",
    "message_file = client.files.create(\n",
    "    file=open(\"./artifacts/War_of_the_Worlds.txt\", \"rb\"), purpose=\"assistants\"\n",
    ")\n",
    "\n",
    "# Create a thread and attach the file to the message\n",
    "thread_with_file_attachment = client.beta.threads.create(\n",
    "    messages=[\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"List all the books you have access to in your files.\",\n",
    "    \n",
    "    # Attach the new file to the message.\n",
    "    \"attachments\": [\n",
    "        { \"file_id\": message_file.id, \"tools\": [{\"type\": \"file_search\"}] }\n",
    "    ],\n",
    "    }\n",
    "]\n",
    ")\n",
    "\n",
    "# The thread now has a vector store with that file in its tool resources.\n",
    "print(thread_with_file_attachment.tool_resources.file_search.vector_store_ids[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Vector Stores at Thread Creation Time\n",
    "\n",
    "Now, let's look at creating a vector store when we create our thread. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread(id='thread_tdjvFdQ8izniY0F8eyZoiYCa', created_at=1717547867, metadata={'can_be_used_for_file_search': 'True', 'has_vector_store': 'True'}, object='thread', tool_resources=ToolResources(code_interpreter=None, file_search=ToolResourcesFileSearch(vector_store_ids=['vs_tCJcmwKhv0YcPN5kxG5MvW8N'])))\n",
      "\n",
      "\n",
      "Thread ID: thread_tdjvFdQ8izniY0F8eyZoiYCa\n",
      "Thread Metadata: {'can_be_used_for_file_search': 'True', 'has_vector_store': 'True'}\n",
      "Thread Tool Resources: ToolResources(code_interpreter=None, file_search=ToolResourcesFileSearch(vector_store_ids=['vs_tCJcmwKhv0YcPN5kxG5MvW8N']))\n"
     ]
    }
   ],
   "source": [
    "thread_with_vector_store = client.beta.threads.create(\n",
    "    messages=[\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"List all the books you have access to in your files. Just give me the list and nothing else.\",\n",
    "    }\n",
    "    ],\n",
    "    tool_resources={\n",
    "            \"file_search\": {\n",
    "                \"vector_stores\": [\n",
    "                    {\n",
    "                        \"file_ids\": [\n",
    "                            dracula_file.id,\n",
    "                            frank_file.id\n",
    "                        ],\n",
    "                        \"metadata\": {\n",
    "                            \"Book1\": \"Dracula\", \n",
    "                            \"Book2\": \"Frankenstein\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        metadata={  # Add metadata about the assistant's capabilities.\n",
    "            \"can_be_used_for_file_search\": \"True\",\n",
    "            \"has_vector_store\": \"True\",\n",
    "        },\n",
    ")\n",
    "\n",
    "# Print the details of the created thread to check its properties\n",
    "print(thread_with_vector_store)  # Print the full thread object.\n",
    "print(\"\\n\")\n",
    "print(\"Thread ID: \" + thread_with_vector_store.id)  # Print the ID of the thread.\n",
    "print(\"Thread Metadata: \" + str(thread_with_vector_store.metadata))  # Print the metadata of the thread.\n",
    "print(\"Thread Tool Resources: \" + str(thread_with_vector_store.tool_resources))  # Print the tool resources of the thread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Results\n",
    "\n",
    "We have built up Assistants and Threads with vector stores and now we want to use them. We need to set up a run to make this happen. Recall there are two approaches: streaming or non-streaming. We will do both.\n",
    "\n",
    "### Streaming Run\n",
    "\n",
    "First, let's do the, more common, streaming run to get our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ASSISTANT MESSAGE >\n",
      "file_search\n",
      "\n",
      "\n",
      "ASSISTANT MESSAGE >\n",
      "The books I have access to in your files are:\n",
      "\n",
      "1. Frankenstein by Mary Wollstonecraft Shelley\n",
      "2. Dracula by Bram Stoker\n",
      "3. I Am Legend by Richard Matheson\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using our first assistant\n",
    "with client.beta.threads.runs.stream(\n",
    "    thread_id=thread_with_vector_store.id,\n",
    "    assistant_id=assistant.id,\n",
    "    instructions=\"You are a helpful assistant. ALWAYS read all the files you have before answering questions about them. Only answer the user's question about the information you have in your files.\",\n",
    "    event_handler=EventHandler(),\n",
    ") as stream:\n",
    "    stream.until_done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Without Streaming\n",
    "\n",
    "Now let's do a run without streaming to compare the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the books available in the files you have uploaded:\n",
      "\n",
      "1. **Alice's Adventures in Wonderland** by Lewis Carroll, including its sequel **Through the Looking-Glass**[0].\n",
      "2. **The Wonderful Wizard of Oz** by L. Frank Baum[1][2].\n",
      "3. **The War of the Worlds** by H.G. Wells[3][4].\n",
      "\n",
      "Feel free to ask questions about any of these books!\n",
      "[0] Alice_in_Wonderland.pdf\n",
      "[1] The_Wonderful_Wizard_of_Oz.txt\n",
      "[2] The_Wonderful_Wizard_of_Oz.txt\n",
      "[3] War_of_the_Worlds.txt\n",
      "[4] War_of_the_Worlds.txt\n"
     ]
    }
   ],
   "source": [
    "# Use the create and poll SDK helper to create a run and poll the status of\n",
    "# the run until it's in a terminal state.\n",
    "\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread_with_file_attachment.id, assistant_id=assistant_with_vector_store.id\n",
    ")\n",
    "\n",
    "messages = list(client.beta.threads.messages.list(thread_id=thread_with_file_attachment.id, run_id=run.id))\n",
    "\n",
    "message_content = messages[0].content[0].text\n",
    "annotations = message_content.annotations\n",
    "citations = []\n",
    "for index, annotation in enumerate(annotations):\n",
    "    message_content.value = message_content.value.replace(annotation.text, f\"[{index}]\")\n",
    "    if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "        cited_file = client.files.retrieve(file_citation.file_id)\n",
    "        citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "print(message_content.value)\n",
    "print(\"\\n\".join(citations))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NormalProgramming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
