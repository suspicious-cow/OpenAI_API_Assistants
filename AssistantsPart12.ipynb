{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 12\n",
    "\n",
    "# Using File Search\n",
    "\n",
    "Universal code for the entire notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai==1.28.1 in e:\\programfiles\\anaconda3\\envs\\normalprogramming\\lib\\site-packages (from -r requirements.txt (line 4)) (1.28.1)\n",
      "Requirement already satisfied: pytz==2023.3 in e:\\programfiles\\anaconda3\\envs\\normalprogramming\\lib\\site-packages (from -r requirements.txt (line 5)) (2023.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in e:\\programfiles\\anaconda3\\envs\\normalprogramming\\lib\\site-packages (from openai==1.28.1->-r requirements.txt (line 4)) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in e:\\programfiles\\anaconda3\\envs\\normalprogramming\\lib\\site-packages (from openai==1.28.1->-r requirements.txt (line 4)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in e:\\programfiles\\anaconda3\\envs\\normalprogramming\\lib\\site-packages (from openai==1.28.1->-r requirements.txt (line 4)) (0.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in e:\\programfiles\\anaconda3\\envs\\normalprogramming\\lib\\site-packages (from openai==1.28.1->-r requirements.txt (line 4)) (1.10.12)\n",
      "Requirement already satisfied: sniffio in e:\\programfiles\\anaconda3\\envs\\normalprogramming\\lib\\site-packages (from openai==1.28.1->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in e:\\programfiles\\anaconda3\\envs\\normalprogramming\\lib\\site-packages (from openai==1.28.1->-r requirements.txt (line 4)) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in e:\\programfiles\\anaconda3\\envs\\normalprogramming\\lib\\site-packages (from openai==1.28.1->-r requirements.txt (line 4)) (4.7.1)\n",
      "Requirement already satisfied: idna>=2.8 in e:\\programfiles\\anaconda3\\envs\\normalprogramming\\lib\\site-packages (from anyio<5,>=3.5.0->openai==1.28.1->-r requirements.txt (line 4)) (3.4)\n",
      "Requirement already satisfied: certifi in e:\\programfiles\\anaconda3\\envs\\normalprogramming\\lib\\site-packages (from httpx<1,>=0.23.0->openai==1.28.1->-r requirements.txt (line 4)) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in e:\\programfiles\\anaconda3\\envs\\normalprogramming\\lib\\site-packages (from httpx<1,>=0.23.0->openai==1.28.1->-r requirements.txt (line 4)) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in e:\\programfiles\\anaconda3\\envs\\normalprogramming\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.28.1->-r requirements.txt (line 4)) (0.14.0)\n",
      "Requirement already satisfied: colorama in e:\\programfiles\\anaconda3\\envs\\normalprogramming\\lib\\site-packages (from tqdm>4->openai==1.28.1->-r requirements.txt (line 4)) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# make sure we all the packages we need\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from openai import OpenAI  # Used for interacting with OpenAI's API\n",
    "from typing_extensions import override  # Used for overriding methods in subclasses\n",
    "from openai import AssistantEventHandler  # Used for handling events related to OpenAI assistants\n",
    "\n",
    "# Additional libraries for time and date manipulation\n",
    "import time\n",
    "import pytz\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the OpenAI class to interact with the API.\n",
    "# This assumes you have set the OPENAI_API_KEY environment variable.\n",
    "client = OpenAI() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Assistant with File Search enabled\n",
    "\n",
    "Our first step is to create an Assistant that can do file searching regardless of where the vector store resides (Assistant or Thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant(id='asst_DJnsWlXrYOT2YlW43BBvht7x', created_at=1717414470, description=None, instructions=' \\n        You are a helpful assistant that answers questions about the stories in your files. The stories are from a variety of authors. \\n        You will answer questions from the user about the stories. All you will do is answer questions about the stories in the files and provide related information.\\n        If the user asks you a question that is not related to the stories in the files, you should let them know that you can only answer questions about the stories.\\n    ', metadata={'can_be_used_for_file_search': 'True', 'can_hold_vector_store': 'True'}, model='gpt-4o', name='File Search Demo Assistant - Stories', object='assistant', tools=[FileSearchTool(type='file_search')], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=None, file_search=ToolResourcesFileSearch(vector_store_ids=[])), top_p=1.0)\n",
      "\n",
      "\n",
      "\n",
      "File Search Demo Assistant - Stories\n",
      "{'can_be_used_for_file_search': 'True', 'can_hold_vector_store': 'True'}\n"
     ]
    }
   ],
   "source": [
    "# Create an assistant using the client library.\n",
    "assistant = client.beta.assistants.create(\n",
    "    model=\"gpt-4o\",  # Specify the model to be used.\n",
    "    \n",
    "    instructions=\"\"\" \n",
    "        You are a helpful assistant that answers questions about the stories in your files. The stories are from a variety of authors. \n",
    "        You will answer questions from the user about the stories. All you will do is answer questions about the stories in the files and provide related information.\n",
    "        If the user asks you a question that is not related to the stories in the files, you should let them know that you can only answer questions about the stories.\n",
    "    \"\"\",\n",
    "    \n",
    "    name=\"File Search Demo Assistant - Stories\",  # Give the assistant a name.\n",
    "    \n",
    "    tools=[{\"type\": \"file_search\"}], # Add the file search capability to the assistant.\n",
    "    \n",
    "    metadata={  # Add metadata about the assistant's capabilities.\n",
    "        \"can_be_used_for_file_search\": \"True\",\n",
    "        \"can_hold_vector_store\": \"True\",\n",
    "    },\n",
    "    temperature=1,  # Set the temperature for response variability.\n",
    "    top_p=1,  # Set the top_p for nucleus sampling.\n",
    ")\n",
    "\n",
    "# Print the details of the created assistant to check its properties.\n",
    "print(assistant)  # Print the full assistant object.\n",
    "print(\"\\n\\n\")\n",
    "print(assistant.name)  # Print the name of the assistant.\n",
    "print(assistant.metadata)  # Print the metadata of the assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Vector Store\n",
    "\n",
    "Now we will create our vector store to hold our files and add files at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "FileCounts(cancelled=0, completed=2, failed=0, in_progress=0, total=2)\n"
     ]
    }
   ],
   "source": [
    "from contextlib import ExitStack\n",
    "\n",
    "# Create a vector store with a name for the store.\n",
    "vector_store = client.beta.vector_stores.create(name=\"Great Fiction Stories\")\n",
    "\n",
    "# Ready the files for upload to the vector store.\n",
    "file_paths = [\"./artifacts/I Am Legend.pdf\", \"./artifacts/The Veldt.pdf\"]\n",
    "\n",
    "# Using ExitStack to manage multiple context managers and ensure they are properly closed.\n",
    "with ExitStack() as stack:\n",
    "    # Open each file in binary read mode and add the file stream to the list\n",
    "    file_streams = [stack.enter_context(open(path, \"rb\")) for path in file_paths]\n",
    "\n",
    "    # Use the upload and poll helper method to upload the files, add them to the vector store,\n",
    "    # and poll the status of the file batch for completion.\n",
    "    file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "        vector_store_id=vector_store.id, files=file_streams\n",
    "    )\n",
    "\n",
    "    # Print the vector store information\n",
    "    print(vector_store.name)\n",
    "    print(vector_store.id)\n",
    "    \n",
    "    # Print the status and the file counts of the batch to see the results\n",
    "    print(file_batch.status)\n",
    "    print(file_batch.file_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attaching the Vector Store to the Assistant\n",
    "\n",
    "We have an Assistant that has File Search enabled and we have a Vector Store with files in them. It's time to join the two up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant Tools:\n",
      " - FileSearchTool(type='file_search')\n",
      "\n",
      "Assistant Tool Resources:\n",
      " - code_interpreter: None\n",
      " - file_search: ToolResourcesFileSearch(vector_store_ids=['vs_xR6pzmCtXQz7iUBrDavNg0Kc'])\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Attach the vector store to the assistant to enable file search capabilities.\n",
    "    assistant = client.beta.assistants.update(\n",
    "        assistant_id=assistant.id,\n",
    "        tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    "    )\n",
    "\n",
    "    # Print the assistant's tools and tool resources to verify the attachment of the vector store.\n",
    "    print(\"Assistant Tools:\")\n",
    "    for tool in assistant.tools:\n",
    "        print(f\" - {tool}\")\n",
    "\n",
    "    # Print the assistant's tool resources to verify the attachment of the vector store\n",
    "    print(\"\\nAssistant Tool Resources:\")\n",
    "    for resource, details in assistant.tool_resources:\n",
    "        print(f\" - {resource}: {details}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while updating the assistant: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Assistant and Vector Store at the Same Time\n",
    "\n",
    "If we have file id's we can just feed them in when creating an assistant to get the Assistant and the Vector Store at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant(id='asst_xDHaF6YQ0fRL1v3Pi33KxYUk', created_at=1717454362, description=None, instructions='You are a helpful assistant that answers questions about the stories in your files. The stories are from a variety of authors. You will answer questions from the user about the stories. All you will do is answer questions about the stories in the files and provide related information. If the user asks you a question that is not related to the stories in the files, you should let them know that you can only answer questions about the stories.', metadata={'can_be_used_for_file_search': 'True', 'can_hold_vector_store': 'True'}, model='gpt-4o', name='Quick Assistant and Vector Store at Once', object='assistant', tools=[FileSearchTool(type='file_search')], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=None, file_search=ToolResourcesFileSearch(vector_store_ids=['vs_dEMiqQ0z81eIU47WGK2HdZmP'])), top_p=1.0)\n",
      "\n",
      "\n",
      "\n",
      "Quick Assistant and Vector Store at Once\n",
      "['vs_dEMiqQ0z81eIU47WGK2HdZmP']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create an assistant using the client library.\n",
    "try:\n",
    "    assistant = client.beta.assistants.create(\n",
    "        model=\"gpt-4o\",  # Specify the model to be used.\n",
    "        instructions=(\n",
    "            \"You are a helpful assistant that answers questions about the stories in your files. \"\n",
    "            \"The stories are from a variety of authors. \"\n",
    "            \"You will answer questions from the user about the stories. All you will do is answer questions about the stories in the files and provide related information. \"\n",
    "            \"If the user asks you a question that is not related to the stories in the files, you should let them know that you can only answer questions about the stories.\"\n",
    "        ),\n",
    "        name=\"Quick Assistant and Vector Store at Once\",  # Give the assistant a name.\n",
    "        tools=[{\"type\": \"file_search\"}],  # Add the file search capability to the assistant.\n",
    "        # Create a vector store and attach it to the assistant in one step.\n",
    "        tool_resources={\n",
    "            \"file_search\": {\n",
    "                \"vector_stores\": [\n",
    "                    {\n",
    "                        \"file_ids\": [\n",
    "                            \"file-UY7uzH3SMK0ALbwQeRk5OE0i\",\n",
    "                            \"file-ZN71rhzhlvLQWiU5ZWEf3WP0\"\n",
    "                        ],\n",
    "                        \"metadata\": {}\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        metadata={  # Add metadata about the assistant's capabilities.\n",
    "            \"can_be_used_for_file_search\": \"True\",\n",
    "            \"can_hold_vector_store\": \"True\",\n",
    "        },\n",
    "        temperature=1,  # Set the temperature for response variability.\n",
    "        top_p=1,  # Set the top_p for nucleus sampling.\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while creating the assistant: {e}\")\n",
    "else:\n",
    "    # Print the details of the created assistant to check its properties.\n",
    "    print(assistant)  # Print the full assistant object.\n",
    "    print(\"\\n\\n\")\n",
    "    print(assistant.name)  # Print the name of the assistant.\n",
    "print(assistant.tool_resources.file_search.vector_store_ids)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NormalProgramming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
