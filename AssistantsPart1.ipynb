{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Assistants Part 1\n",
    "The Assistants API allows you to build AI assistants within your own applications. An Assistant has instructions and can leverage models, tools, and files to respond to user queries. The Assistants API currently supports three types of tools: Code Interpreter, File Search, and Function calling.\n",
    "\n",
    "A typical integration of the Assistants API has the following flow:\n",
    "\n",
    "1. Create an Assistant by defining its custom instructions and picking a model. If helpful, add files and enable tools like Code Interpreter, File Search, and Function calling.\n",
    "2. Create a Thread when a user starts a conversation.\n",
    "3. Add Messages to the Thread as the user asks questions.\n",
    "4. Run the Assistant on the Thread to generate a response by calling the model and the tools.\n",
    "\n",
    "## Calling Pre-Existing Assistants with Code\n",
    "To use a preexisting Assistant we have to get the Assistant id first. We can do this through the interface or through code. Below we show how to do this with code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Assistant(id='asst_jrcAIIKpz1O8s5GKuMMKawAx', created_at=1714348610, description=None, instructions='You are a helpful assistant.', metadata={}, model='gpt-4-turbo', name='Function Fun', object='assistant', tools=[FunctionTool(function=FunctionDefinition(name='get_weather', description='Determine weather in my location', parameters={'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'enum': ['c', 'f']}}, 'required': ['location']}), type='function')], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=None, file_search=None), top_p=1.0), Assistant(id='asst_sZiMAv1OmVpN464k2JoxyjdB', created_at=1714348320, description=None, instructions='You are a helpful assistant.', metadata={}, model='gpt-4-turbo', name='Code Brutha', object='assistant', tools=[CodeInterpreterTool(type='code_interpreter')], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=ToolResourcesCodeInterpreter(file_ids=['file-zANRT5ylCOIKzeFzV3HpJBru']), file_search=None), top_p=1.0), Assistant(id='asst_hlsbSbcxUitk2IF36G5KOi0H', created_at=1714347804, description=None, instructions='You are a helpful assistant that answers questions based on the files that you have available to you. ', metadata={}, model='gpt-4-turbo', name='File Search Assistant', object='assistant', tools=[FileSearchTool(type='file_search')], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=None, file_search=ToolResourcesFileSearch(vector_store_ids=['vs_3OD2BXxllkqiuVflGii9woHS'])), top_p=1.0), Assistant(id='asst_2Y9kU1ZtaqtHNbNm83s78CV1', created_at=1714347606, description=None, instructions='Vox Antiqua excels in translating any phrase into Ancient Latin, emphasizing adaptation to fit the cultural and historical context of the time. It aims to provide translations that reflect how the phrase would have been expressed authentically in ancient times, rather than just literal translations. This approach involves a deep understanding of the nuances of both the source language and Ancient Latin, including idiomatic, historical, literary, or everyday expressions. Vox Antiqua avoids modern interpretations unless specifically requested. For ambiguous or unclear inputs, it seeks clarification to ensure accuracy. It avoids engaging with inappropriate content or topics that do not align with its purpose of translation.', metadata={}, model='gpt-4-turbo', name='Vox Antiqua', object='assistant', tools=[], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=None, file_search=None), top_p=1.0)]\n",
      "\n",
      "\n",
      "\n",
      "Name: Function Fun, ID: asst_jrcAIIKpz1O8s5GKuMMKawAx\n",
      "Name: Code Brutha, ID: asst_sZiMAv1OmVpN464k2JoxyjdB\n",
      "Name: File Search Assistant, ID: asst_hlsbSbcxUitk2IF36G5KOi0H\n",
      "Name: Vox Antiqua, ID: asst_2Y9kU1ZtaqtHNbNm83s78CV1\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "my_assistants = client.beta.assistants.list(\n",
    "    order=\"desc\",\n",
    "    limit=\"20\",\n",
    ")\n",
    "\n",
    "print(my_assistants.data)\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# Iterate through each assistant in the list and print its name and ID\n",
    "for assistant in my_assistants.data:\n",
    "    print(f\"Name: {assistant.name}, ID: {assistant.id}\")\n",
    "\n",
    "print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can retrieve the Assistant by the id number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant(id='asst_2Y9kU1ZtaqtHNbNm83s78CV1', created_at=1714347606, description=None, instructions='Vox Antiqua excels in translating any phrase into Ancient Latin, emphasizing adaptation to fit the cultural and historical context of the time. It aims to provide translations that reflect how the phrase would have been expressed authentically in ancient times, rather than just literal translations. This approach involves a deep understanding of the nuances of both the source language and Ancient Latin, including idiomatic, historical, literary, or everyday expressions. Vox Antiqua avoids modern interpretations unless specifically requested. For ambiguous or unclear inputs, it seeks clarification to ensure accuracy. It avoids engaging with inappropriate content or topics that do not align with its purpose of translation.', metadata={}, model='gpt-4-turbo', name='Vox Antiqua', object='assistant', tools=[], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=None, file_search=None), top_p=1.0)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "my_assistant = client.beta.assistants.retrieve(\"asst_2Y9kU1ZtaqtHNbNm83s78CV1\")\n",
    "print(my_assistant)\n",
    "print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Minimal Assistant\n",
    "Let's begin by creating an Assistant through code. As a reminder, you can also create Assistants through the user interface at https://platform.openai.com/assistants as well. \n",
    "\n",
    "**NOTE: Make sure you have an environment variable, called OPENAI_API_KEY set with your API key before running this code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are only required to pass in a model\n",
    "# The rest of the parameters are optional\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "  model=\"gpt-4-turbo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant(id='asst_YfqFWQ0QDDasnqm75Limnrto', created_at=1714385618, description=None, instructions=None, metadata={}, model='gpt-4-turbo', name=None, object='assistant', tools=[], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=None, file_search=None), top_p=1.0)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List the latest assistant added\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "my_assistants = client.beta.assistants.list(\n",
    "    order=\"desc\",\n",
    "    limit=\"20\",\n",
    ")\n",
    "print(my_assistants.data[0])\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Basic Assistant\n",
    "\n",
    "Now let's create an actual Assistant with the following arguments:\n",
    "\n",
    "**model**\n",
    "(string)\n",
    "\n",
    "*Required*\n",
    "\n",
    "ID of the model to use. You can use the List models API to see all of your available models, or see our Model overview for descriptions of them.\n",
    "\n",
    "---\n",
    "\n",
    "**name**\n",
    "(string or null)\n",
    "\n",
    "*Optional*\n",
    "\n",
    "The name of the assistant. The maximum length is 256 characters.\n",
    "\n",
    "---\n",
    "\n",
    "**description**\n",
    "(string or null)\n",
    "\n",
    "*Optional*\n",
    "\n",
    "The description of the assistant. The maximum length is 512 characters.\n",
    "\n",
    "---\n",
    "\n",
    "**instructions**\n",
    "(string or null)\n",
    "\n",
    "*Optional*\n",
    "\n",
    "The system instructions that the assistant uses. The maximum length is 256,000 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant(id='asst_znZB6pch1MnjziQEYzvY1d0w', created_at=1714385619, description='This is a seriously cool assistant', instructions='You are a helpful assistant.', metadata={}, model='gpt-4-turbo', name='Seriously Cool Assistant', object='assistant', tools=[], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=None, file_search=None), top_p=1.0)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The actual minimal arguments \"required\" for creating an assistant\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "  model=\"gpt-4-turbo\",\n",
    "  name=\"Seriously Cool Assistant\",\n",
    "  description=\"This is a seriously cool assistant\",\n",
    "  instructions=\"You are a helpful assistant.\",\n",
    ")\n",
    "\n",
    "print(assistant)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Arguments\n",
    "**metadata**\n",
    "(map)\n",
    "\n",
    "*Optional*\n",
    "Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format. Keys can be a maximum of 64 characters long and values can be a maxium of 512 characters long.\n",
    "\n",
    "---\n",
    "\n",
    "**temperature**\n",
    "(number or null)\n",
    "\n",
    "*Optional*\n",
    "(Defaults to 1)\n",
    "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n",
    "\n",
    "---\n",
    "\n",
    "**top_p**\n",
    "(number or null)\n",
    "\n",
    "*Optional*\n",
    "(Defaults to 1)\n",
    "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n",
    "\n",
    "---\n",
    "\n",
    "**response_format**\n",
    "(string or object)\n",
    "\n",
    "*Optional*\n",
    "(Defaults to \"auto\")\n",
    "Specifies the format that the model must output. Compatible with GPT-4 Turbo and all GPT-3.5 Turbo models since gpt-3.5-turbo-1106.\n",
    "\n",
    "Setting to { \"type\": \"json_object\" } enables JSON mode, which guarantees the message the model generates is valid JSON.\n",
    "\n",
    "Important: when using JSON mode, you must also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly \"stuck\" request. Also note that the message content may be partially cut off if finish_reason=\"length\", which indicates the generation exceeded max_tokens or the conversation exceeded the max context length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant(id='asst_iPyZB0ihlobpRUlm8eO6SKfH', created_at=1714385620, description='This assistant has extra arguments', instructions='You are a helpful assistant.', metadata={'key': 'value', 'IsAwesome': 'True', 'IsBeta': 'False'}, model='gpt-4-turbo', name='Extra Arguments Assistant', object='assistant', tools=[], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=None, file_search=None), top_p=1.0)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the extra arguments\n",
    "# WARNING: Make sure you have the latest version of the OpenAI Python SDK\n",
    "# Use the following command to install the latest version:\n",
    "# pip install openai --upgrade\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "  model=\"gpt-4-turbo\",\n",
    "  name=\"Extra Arguments Assistant\",\n",
    "  description=\"This assistant has extra arguments\",\n",
    "  instructions=\"You are a helpful assistant.\",\n",
    "  metadata={\n",
    "      \"key\": \"value\", \n",
    "      \"IsAwesome\": \"True\", \n",
    "      \"IsBeta\": \"False\"\n",
    "      },\n",
    "  temperature=1,\n",
    "  top_p=1,\n",
    "  response_format=\"auto\",\n",
    ")\n",
    "\n",
    "print(assistant)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Tools\n",
    "**tools**\n",
    "(array)\n",
    "\n",
    "*Optional*\n",
    "(Defaults to [])\n",
    "A list of tool enabled on the assistant. There can be a maximum of 128 tools per assistant. Tools can be of types code_interpreter, file_search, or function.\n",
    "\n",
    "---\n",
    "\n",
    "**tool_resources**\n",
    "(object or null)\n",
    "\n",
    "*Optional*\n",
    "A set of resources that are used by the assistant's tools. The resources are specific to the type of tool. For example, the code_interpreter tool requires a list of file IDs, while the file_search tool requires a list of vector store IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant(id='asst_N7YKItx73QyjUjAIK1zRWbWc', created_at=1714385620, description=None, instructions='You are a personal data analyst. When asked a question, write and run Python code to answer the question.', metadata={'key': 'value', 'IsAwesome': 'True', 'IsBeta': 'False'}, model='gpt-4-turbo', name='Code Interpreter Assistant', object='assistant', tools=[CodeInterpreterTool(type='code_interpreter')], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=ToolResourcesCodeInterpreter(file_ids=[]), file_search=None), top_p=1.0)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an assistant that uses the code interpreter tool\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "    instructions=\"You are a personal data analyst. When asked a question, write and run Python code to answer the question.\",\n",
    "    name=\"Code Interpreter Assistant\",\n",
    "    metadata={\n",
    "        \"key\": \"value\", \n",
    "        \"IsAwesome\": \"True\", \n",
    "        \"IsBeta\": \"False\"\n",
    "        },\n",
    "    temperature=1,\n",
    "    top_p=1,\n",
    "    response_format=\"auto\",\n",
    "    tools=[{\"type\": \"code_interpreter\"}],\n",
    "    tool_resources=None,\n",
    "    model=\"gpt-4-turbo\",\n",
    ")\n",
    "\n",
    "print(assistant)\n",
    "print(\"\\n\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NormalProgramming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
